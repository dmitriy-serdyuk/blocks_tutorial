{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy\n",
    "import theano\n",
    "from theano import config\n",
    "from theano import tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blocks tutorial\n",
    "For more information see [documentation](http://blocks.readthedocs.org/en/latest) and [examples](https://github.com/mila-udem/blocks-examples/).\n",
    "\n",
    "### Note\n",
    "Blocks is in early stage of development and the interface is a subject to regular changes. You can use the [stable version](https://github.com/mila-udem/blocks/releases/tag/v0.0.1) but the development version will always have more functionality.\n",
    "\n",
    "We appreciate bug reports, pull requests, constructive criticism and answer questions. The mailing list can be found [here](https://groups.google.com/forum/#!forum/blocks-users)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bricks\n",
    "\n",
    "### Basic Concepts\n",
    "\n",
    "_Bricks_ are \"parametrized ops\". They can build Theano graphs given input variables.\n",
    "The process is called _application of a brick_. Let us create a simple ``Linear`` brick and apply it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blocks.bricks import Linear\n",
    "x = tensor.matrix('features') # dimensions: (batch, features)\n",
    "linear = Linear(input_dim=784, output_dim=10)\n",
    "# Applying a brick yields a (several) Theano variable(s)\n",
    "linear_output = linear.apply(x)\n",
    "print(type(linear_output))\n",
    "# With the returned variable you can do whatever \n",
    "# you usually do with Theano variables\n",
    "y_hat = abs(2 * linear_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above ``linear.apply`` is an _application method_. A brick might have several application methods, see e.g. ``Softmax``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blocks.bricks import Softmax\n",
    "# The application methods are decorated by `@application` decorator in the code\n",
    "# and in fact are somewhat more complex than usual python functions:\n",
    "print(Softmax.categorical_cross_entropy)\n",
    "print(Softmax.apply)\n",
    "print(Softmax.__init__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a step back and take a look at the computation graph that we have built to see that it matches our expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theano.printing.debugprint(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that $\\hat{y} = |2(xW + b)|$ as expected. Note, that the parameters $W$ and $b$ were automatically created by the ``linear`` object. The parameters are also accessible as ``linear.parameters``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linear.parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Annotated graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might wonder why the debugprint above contains two ``Elemwise{identity}`` nodes. This is because bricks build  _annotated_ computation graphs. Variables created by bricks are heavily tagged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linear_output.tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bricks insert special variables in the computation graph that carry these tags. This is done by calling ``TensorVarible.copy`` method which works as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_copy = x.copy(name='x_copy')\n",
    "# The new variable is an output of a new Elemwise{identity} node\n",
    "theano.printing.debugprint(x_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's why you have so many ``Elemwise{identity}`` nodes in your graphs created by bricks.\n",
    "\n",
    "Let's go through the content of the tags added by bricks. For one, each variable in addition to a Theano-level name gets a brick-level name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(linear_output.tag.name)\n",
    "print(linear_output.owner.inputs[0].owner.inputs[0].owner.inputs[0].tag.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, each variable, including parameters, is assigned a (several) role(s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(linear.parameters[0].tag.roles)\n",
    "print(linear.parameters[1].tag.roles)\n",
    "print(linear_output.tag.roles)\n",
    "print(linear_output.owner.inputs[0].owner.inputs[0].owner.inputs[0].tag.roles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the ``annotations`` attribute of the tag contains references to the brick, that created the variable and the ``ApplicationCall`` object. For now, all we need to know about the latter is that it was created when ``linear.apply`` was called and that it refers by name to the application method that created it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(linear_output.tag.annotations)\n",
    "print(linear_output.tag.annotations[0] == linear)\n",
    "print(linear_output.tag.annotations[1].application.application.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make exploration of the computation graph easier, variables created by bricks are assigned names composed from the name of the brick, the name of the application method and the local (``.tag.name``) variable name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(linear_output.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see these names when you use `theano.printing.debugprint`.\n",
    "\n",
    "##### Lazy initialization\n",
    "\n",
    "Finally, let's try do the forward pass using the computatation graph $\\hat{y}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = theano.function([x], y_hat)\n",
    "f(numpy.ones((10, 784), dtype=config.floatX))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops! Not so surprising, because we did not tell ``linear`` how to initialize its parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blocks.initialization import Constant\n",
    "linear.weights_init = Constant(2.)\n",
    "linear.biases_init = Constant(0.)\n",
    "linear.initialize()\n",
    "f(numpy.ones((3, 784), dtype=config.floatX))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it works. Alternatively, you could pass ``weights_init`` and ``biases_init`` settings at the construction time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linear = Linear(input_dim=784, output_dim=10, \n",
    "                weights_init=Constant(2), biases_init=Constant(0))\n",
    "linear.initialize()\n",
    "f = theano.function([x], linear.apply(x))\n",
    "f(numpy.ones((3, 784), dtype=config.floatX))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We deliberately allow to create the brick first and set the initialization schemes later. This is called _lazy initialization_ and implemented by decorating all brick constructors with `@lazy`. It is important to\n",
    "explicitly call ``.initialize()`` method if you train from scratch, and you can skip it when you continue training from loaded parameters.\n",
    "\n",
    "##### Nested bricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Linear`` is a very simple brick. More complex bricks often use other bricks to build parts of their computation graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from blocks.bricks import MLP, Tanh, Softmax\n",
    "mlp = MLP([Tanh(), Softmax()], [784, 100, 10])\n",
    "probs = mlp.apply(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bricks to be used are either passed at the creation time or created by the brick itself, also at the creation time. A brick that is using other bricks is called a _parent_, and the bricks used are called _children_. Each brick keeps track of its children and parents. In theory a brick can be a child of multiple bricks, it practice it sufficient to have tree-like brick hierarchies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The children of `mlp` are the linear bricks that it created plus\n",
    "# the activations that we created.\n",
    "print(mlp.children)\n",
    "# `mlp` is not a child of any brick => it does not have a parent\n",
    "print(mlp.parents)\n",
    "# `Softmax()` is a basic brick and does not have children\n",
    "print(mlp.children[-1].children)\n",
    "# but it does have a parent, which is `mlp`\n",
    "print(mlp.children[-1].parents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the new computation graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theano.printing.debugprint(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that indeed all ``mlp.children`` were applied sequentially. Note how annotations help us read the computation graph!\n",
    "\n",
    "Your might wonder how ``Linear`` children of ``mlp`` will initialize their parameters. No worries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blocks.bricks import Initializable\n",
    "# All bricks that inherit from Initializable\n",
    "# - have \"weights_init\" and \"biases_init\" properties\n",
    "# - push them to children  before initialization\n",
    "print(isinstance(mlp, Initializable))\n",
    "mlp.weights_init = Constant(4.)\n",
    "mlp.biases_init = Constant(0.)\n",
    "mlp.initialize()\n",
    "# Let's check that \"weights_init\" was indeed propagated\n",
    "print(mlp.children[0].weights_init)\n",
    "print(mlp.children[0].parameters[0].get_value().sum() == 784 * 100 * 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All right, but what if you want to initialize the linear layers differently? The way to go is to trigger configuration pushing _before_ initialization, so that you could hack the configuration of children in between:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp = MLP([Tanh(), Softmax()], [784, 100, 10], \n",
    "          weights_init=Constant(4.), biases_init=Constant(0.))\n",
    "# Each brick has a `push_initialization_config` method that pushes \n",
    "# initialization settings downward. It is only called once, so when you\n",
    "# trigger it explicitly, it will not be called in `initialize`.\n",
    "mlp.push_initialization_config()\n",
    "mlp.linear_transformations[1].weights_init = Constant(2.)\n",
    "mlp.initialize()\n",
    "print(mlp.linear_transformations[0].parameters[0].get_value().sum() == 784 * 100 * 4)\n",
    "print(mlp.linear_transformations[1].parameters[0].get_value().sum() == 10 * 100 * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Basic concepts summary\n",
    "\n",
    "We have introduced the following concepts: \n",
    "\n",
    "- bricks\n",
    "- application methods\n",
    "- brick parameters\n",
    "- annotated computation graph\n",
    "- lazy initialization\n",
    "- children and parents of a brick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of available bricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Basic bricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the basic bricks you can e.g. build a classifier for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blocks.initialization import IsotropicGaussian\n",
    "mlp = MLP([Tanh(), Softmax()], [784, 100, 10],\n",
    "          weights_init=IsotropicGaussian(0.01),\n",
    "          biases_init=Constant(0))\n",
    "mlp.initialize()\n",
    "probs = mlp.apply(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train ``mlp`` you need to compute the cross-entropy cost. The Blocks way to do that is to use another brick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blocks.bricks.cost import CategoricalCrossEntropy\n",
    "y = tensor.lmatrix('targets') # dimensions: (batch, 1)\n",
    "cost = CategoricalCrossEntropy().apply(y.flatten(), probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of the cost above is not numerically stable unless Theano optimizer is very smart. To our knowledge it is not yet smart enough, and to ensure that you do not have numerical instability in your training procedure you should use ``Softmax.categorical_cross_entropy``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Softmax() is moved out of MLP\n",
    "mlp = MLP([Tanh(), None], [784, 100, 10],\n",
    "          weights_init=IsotropicGaussian(0.01),\n",
    "          biases_init=Constant(0))\n",
    "mlp.initialize()\n",
    "softmax = Softmax()\n",
    "# This never computes the probabilities, only their logarithms.\n",
    "# No NaNs in the gradients!\n",
    "cost = softmax.categorical_cross_entropy(y.flatten(), mlp.apply(x)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some other basic bricks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# activations\n",
    "from blocks.bricks import Logistic, Tanh, Rectifier\n",
    "# maxout related bricks\n",
    "from blocks.bricks import Maxout, LinearMaxout\n",
    "# for e.g. word embeddings\n",
    "from blocks.bricks.lookup import LookupTable\n",
    "# simply chains several bricks\n",
    "from blocks.bricks import Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convolutional bricks\n",
    "\n",
    "The bricks that build convolutional networks are not that much different from the basic ones.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convolutions, subsampling\n",
    "from blocks.bricks.conv import Convolutional, MaxPooling\n",
    "# and classes which construct multi layer CNNs (like MLP)\n",
    "from blocks.bricks.conv import ConvolutionalSequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See our [LeNet demo](https://github.com/mila-udem/blocks-examples/blob/master/mnist_lenet/__init__.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Recurrent bricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We have a standard set of recurrent networks\n",
    "from blocks.bricks.recurrent import SimpleRecurrent, GatedRecurrent, LSTM\n",
    "# plus a brick than can stack a few of them!\n",
    "from blocks.bricks.recurrent import RecurrentStack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [tutorial](https://blocks.readthedocs.org/en/latest/) and the [parity problem demo](https://github.com/mila-udem/blocks-examples/tree/master/parity_problem)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sequence generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``SequenceGenerator`` is a high-level brick that can be used to implement language models and Encoder-Decoders, with and without attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from blocks.bricks.sequence_generators import SequenceGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [the extensive API documentation](https://blocks.readthedocs.org/en/latest/api/bricks.html#blocks.bricks.sequence_generators.BaseSequenceGenerator), [Markov chain demo](https://github.com/mila-udem/blocks-examples/tree/master/markov_chain), [reverse words demo](https://github.com/mila-udem/blocks-examples/tree/master/reverse_words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building your own bricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing a new brick is very easy when you know that it will neither be a child not a parent of any other brick. Consider an example of a brick that computes $x W x^T + xb$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blocks.bricks.base import application\n",
    "from blocks.utils import shared_floatx_nans\n",
    "\n",
    "# Inheriting from Initializable gives us \n",
    "# - lazy `weights_init` and `biases_init` attributes\n",
    "# - `rng` attribute which is the random number generator \n",
    "#    that should be used to actually initialize parameters.\n",
    "class Quadratic(Initializable):\n",
    "    def __init__(self, input_dim, **kwargs):\n",
    "        # Do not forget to call super()!!!\n",
    "        super(Quadratic, self).__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "    # You must put the code that creates shared variables\n",
    "    # for parameters in `_allocate` method. This requirement\n",
    "    # comes from the \"lazy allocation\" feature which we do not discuss today.\n",
    "    def _allocate(self):\n",
    "        self.parameters = [\n",
    "            shared_floatx_nans((self.input_dim, self.input_dim), name='W'),\n",
    "            shared_floatx_nans((self.input_dim), name='b')]      \n",
    "        \n",
    "    # You must put your actual initialization code \n",
    "    # in your `_initialize` method for lazy initialization to work\n",
    "    def _initialize(self):\n",
    "        self.weights_init.initialize(self.parameters[0], self.rng)\n",
    "        self.biases_init.initialize(self.parameters[1], self.rng)\n",
    "        \n",
    "    # It is the `@application` decorator that actually takes\n",
    "    # care of tagging input and output variables. The `inputs`\n",
    "    # and `outputs` arguments define the brick-level names \n",
    "    # of inputs and outputs respectively. \n",
    "    @application(inputs=['input_'], outputs=['output'])\n",
    "    def apply(self, input_):\n",
    "        return (input_.dot(self.parameters[0]).dot(input_.transpose()) + \n",
    "                input_.dot(self.parameters[1]))\n",
    "                \n",
    "quadratic = Quadratic(input_dim=2, \n",
    "                      weights_init=Constant(2), biases_init=Constant(1))\n",
    "quadratic.initialize()\n",
    "result = quadratic.apply(x)\n",
    "f = theano.function([x], [result])\n",
    "print(f(3 * numpy.ones((1, 2), dtype=config.floatX))[0] ==  3 ** 2 * 2 * 2 * 2 + 3 * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing a new brick that works well in hierarchies requires deeper understanding of brick life-cycle, which is slightly out of the scope of this tutorial. You might find the [existing brick tutorial](http://blocks.readthedocs.org/en/latest/bricks_overview.html) and the  [work-in-progress brick development tutorial](https://github.com/mila-udem/blocks/pull/772) useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph filtering and modifications\n",
    "\n",
    "At this point you might wonder what is the benefit from annotating the graph. The following are the most important usecases for the annotations:\n",
    "\n",
    "- you can extract inner variables from the graph and use them for debugging, monitoring or constructing various additive penalties (L1, L2, etc.)\n",
    "- you can replace inner variables to implement advanced regularization such as dropout, weight noise addition, batch normalization (see [the PR](https://github.com/mila-udem/blocks/pull/851))\n",
    "- the computation graph simply becomes more readable (looking forward for visualizations taking use of annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two main classes that you need to work with annotated graphs are ``ComputationGraph`` and ``VariableFilter``. Here is a simple example of applying L2 regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blocks.graph import ComputationGraph\n",
    "from blocks.filter import VariableFilter\n",
    "from blocks.roles import WEIGHT\n",
    "\n",
    "cg = ComputationGraph([cost])\n",
    "# cg.variables is simply the list of all graph variables\n",
    "W1, W2 = VariableFilter(roles=[WEIGHT])(cg.variables)\n",
    "cost = cost + .00005 * (W1 ** 2).sum() + .00005 * (W2 ** 2).sum()\n",
    "cost.name = 'training_cost'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``VariableFilter`` can filter by\n",
    "- the roles of a variable (``INPUT``, ``OUTPUT``, ``WEIGHT``, ...)\n",
    "- the bricks and the application method that created a variable\n",
    "- the variable names\n",
    "    \n",
    "Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blocks.roles import OUTPUT\n",
    "print(VariableFilter(bricks=[mlp.linear_transformations[0]])(cg.variables))\n",
    "layer1_activations, = VariableFilter(\n",
    "    roles=[OUTPUT], bricks=[mlp.activations[0]])(cg.variables)\n",
    "print(layer1_activations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first example we found parameters, input, outputs and also _auxiliary_ variables that ``linear_transformations[0]`` created.\n",
    "\n",
    "In the second example we fetched activations of the first layer. Let's try to do something useful with them, e.g. apply dropout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blocks.graph import apply_dropout\n",
    "cg_dropout = apply_dropout(cg, [layer1_activations], 0.5)\n",
    "\n",
    "# Let's check what has been done\n",
    "layer2_activations = VariableFilter(\n",
    "    roles=[OUTPUT], bricks=[mlp.linear_transformations[1]])(cg_dropout.variables)\n",
    "theano.printing.debugprint(layer2_activations)                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally `apply_dropout` uses `ComputationGraph.replace` which is a wrapper for ``theano.clone``. It is smarter than ``theano.clone`` and handle several interdependent replacements.\n",
    "\n",
    "##### Remarks\n",
    "- we were able to apply L2 and dropout regularization to ``MLP`` without any explicit support on its side \n",
    "- if for example we had a ``Dropout`` brick, we would not be able to insert it into ``MLP``\n",
    "\n",
    "This was to highlight the main advantage of our \"search-and-replace\" approach: we can keep our bricks plain and simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms\n",
    "Blocks has a collection of algorihms and allows to use arbitary combinations of them. Note, that this part of the library is almost standalone.\n",
    "\n",
    "The heart of blocks/algorithms is the ``GradientDescent`` class. Do not be confused by the name, ``GradientDescent`` is very generic! It computes gradients and feeds them to a ``StepRule`` object and at every iteration subtracts the step proposed by the step rule.\n",
    "\n",
    "Let's make one SGD step with learning rate $0.1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blocks.algorithms import GradientDescent, Scale\n",
    "algorithm = GradientDescent(\n",
    "    cost=cost, parameters=cg.parameters,\n",
    "    step_rule=Scale(learning_rate=0.1))\n",
    "# This call compiles the Theano function\n",
    "algorithm.initialize()\n",
    "# This makes one parameter update.\n",
    "# Note, that the input of process_batch must be a dictionary\n",
    "# with the variable names as keys.\n",
    "algorithm.process_batch(\n",
    "    {'features' : numpy.ones((10, 784), dtype=config.floatX), \n",
    "     'targets' : numpy.ones((10, 1), dtype='int64')})\n",
    "# You can check which updates are performed at each step\n",
    "print(algorithm.updates)\n",
    "# Uncomment the following line if you want to make sure that the update is right\n",
    "# theano.printing.debugprint(algorithm.updates[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the following step rules:\n",
    "\n",
    "- `Scale` -- scales its input, if it is a single rule applied to gradients it is SGD\n",
    "- `CompositeRule` -- a rule used to compose several rules to a sequence\n",
    "- `Momentum` -- adds momentum (SGD-Momentum, but can be applied to any kind of rule)\n",
    "- `AdaDelta` -- adaptive learning rate AdaDelta algorithm\n",
    "- `RMSProp` -- another learning algorithm\n",
    "- `Adam` -- one more algorithm\n",
    "- `StepClipping` -- clips the step, can be used for gradient clipping if applied before or for step clipping if applied after other rules\n",
    "- and others, see [documentation](http://blocks.readthedocs.org/en/latest/api/algorithms.html)\n",
    "\n",
    "Here is an example of step rule composition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from blocks.algorithms import CompositeRule, StepClipping\n",
    "gradient_clipping = CompositeRule([StepClipping(threshold=1.0), Scale(learning_rate=0.1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main loop\n",
    "\n",
    "The main loop in Blocks manages the training process and glues up an algorithm, a data stream and extensions. Main loop itself is a very simple object, it fetches data from a datastream and feeds it to a training algorithm. All additional functionality is added with extensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A minimal example of a main loop requires a dataset, we'll use MNIST dataset from fuel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fuel.datasets.mnist import MNIST\n",
    "mnist_train = MNIST((\"train\",))\n",
    "mnist_test = MNIST((\"test\",))\n",
    "mnist_train.sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that we providently defined our input variables $x$ and $y$ with the same names as dataset sources. This will allow the algorithm to parse batches produced by the data stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some Fuel magic, we'll get a stream of batches of size 50\n",
    "from fuel.transformers import Flatten\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import SequentialScheme\n",
    "\n",
    "train_stream = Flatten(\n",
    "    DataStream.default_stream(\n",
    "        mnist_train,\n",
    "        iteration_scheme=SequentialScheme(\n",
    "            mnist_train.num_examples, 50)),\n",
    "    which_sources=('features',))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to construct our first main loop. But even the termination of the main loop is handled by an extension, so the plain main loop will never finish:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blocks.main_loop import MainLoop\n",
    "\n",
    "# You have to provide a fresh algorithm object every time \n",
    "# you create a new main loop. \n",
    "algorithm = GradientDescent(\n",
    "    cost=cost, parameters=cg.parameters,\n",
    "    step_rule=Scale(learning_rate=0.1))\n",
    "main_loop = MainLoop(algorithm, train_stream)\n",
    "# If you run this, it will never finish\n",
    "# main_loop.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's proceed to a more sensible example which terminates (thanks to `FinishAfter`) and prints something (thanks to `Printing`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blocks.model import Model\n",
    "from blocks.extensions import FinishAfter, Printing\n",
    "\n",
    "# Reinitialize algorithm\n",
    "algorithm = GradientDescent(\n",
    "    cost=cost, parameters=cg.parameters,\n",
    "    step_rule=Scale(learning_rate=0.1))\n",
    "# Define the main loop, `Model` is a very simple wrapper of the computational graph\n",
    "main_loop = MainLoop(\n",
    "    algorithm,\n",
    "    train_stream,\n",
    "    model=Model(cost),\n",
    "    extensions=[\n",
    "        Printing(),\n",
    "        FinishAfter(after_n_batches=2)])\n",
    "# And run it!\n",
    "main_loop.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging\n",
    "In the example above the printing extension prints the values from the log. You can access the log like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "main_loop.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log is a dictionary which maps from iteration number to a dictionary of log records. Each record is a pair of record name and its value.\n",
    "\n",
    "As you may have seen in the previous example, log also contains status information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "main_loop.log.status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log has an sqlite backend, with this backend it can store only simple types like boolean, numberical, or string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring\n",
    "\n",
    "There are two types of monitoring in Blocks: training monitoring and data stream monitoring. The first one computes the monitored values during the training and uses training batches. This type of monitoring almost doesn't take time. The data stream monitoring iterates over the dataset and computes the monitored quantities. It may take some time to iterate over a big dataset. \n",
    "\n",
    "The first type usually used for approximate train subset monitoring and the second one for validation/test subsets. However, you can use `DataStreamMonitoring` for the train subset to get not averaged values.\n",
    "\n",
    "The following example shows regular usage of monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blocks.extensions.monitoring import DataStreamMonitoring, TrainingDataMonitoring\n",
    "from blocks.bricks.cost import MisclassificationRate\n",
    "\n",
    "error_rate = MisclassificationRate().apply(y.flatten(), probs).copy(name='error_rate')\n",
    "\n",
    "train_monitoring = TrainingDataMonitoring(\n",
    "    [cost, error_rate], prefix='train', after_batch=True)\n",
    "test_monitoring = DataStreamMonitoring(\n",
    "    [cost, error_rate], \n",
    "    Flatten(\n",
    "        DataStream.default_stream(\n",
    "            mnist_test,\n",
    "            iteration_scheme=SequentialScheme(\n",
    "                mnist_test.num_examples, 500)),\n",
    "        which_sources=('features',)), \n",
    "    prefix='test',\n",
    "    after_batch=True)\n",
    "print(train_monitoring.record_name(cost))\n",
    "print(test_monitoring.record_name(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to combine values you can use aggregations schemes. For example for the mean gradient norm you can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blocks.monitoring import aggregation\n",
    "average_train_monitoring = TrainingDataMonitoring(\n",
    "    [cost, error_rate, \n",
    "     aggregation.mean(algorithm.total_gradient_norm).copy(name='mean_total_grad_norm'),\n",
    "     ], every_n_batches=2)\n",
    "train_monitoring = TrainingDataMonitoring(\n",
    "    [algorithm.total_gradient_norm.copy(name='total_grad_norm')], after_batch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally the values are aggregated using a shared theano variable. It means that the shape of the aggregated value should be constant. In a case if you would like to monitor all the activations, for example, make sure that your batch size is constant and the time series have the same length.\n",
    "\n",
    "Gradient descent and step rules have fields which contain variables which can be monitored: total gradient norm, total step norm, learning rate, momentum, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blocks.extensions import Printing\n",
    "# Reinitialize algorithm\n",
    "algorithm = GradientDescent(\n",
    "    cost=cost, parameters=cg.parameters,\n",
    "    step_rule=Scale(learning_rate=0.1))\n",
    "\n",
    "main_loop = MainLoop(\n",
    "        algorithm,\n",
    "        train_stream,\n",
    "        model=Model(cost),\n",
    "        extensions=[FinishAfter(after_n_batches=4), \n",
    "                    train_monitoring, average_train_monitoring, test_monitoring, \n",
    "                    Printing(after_batch=True)])\n",
    "main_loop.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the aggregation scheme averaged the gradient norm.\n",
    "\n",
    "### Serialization\n",
    "We tried to make serialization to work with any kind of user code and to be easy enough deserialized on different types of hardware. We partly addressed these tasks using `cPickle` and saving Theano shared variables as numpy array with meta information (we use persistent ids internally). Serialization is being refactored now, so pay attention to the mailing list.\n",
    "\n",
    "`blocks.serialization` module provides functions `dump`, `sequre_dump`, `load`, and `continue_training`.\n",
    "\n",
    "`Checkpoint` is an extension which serializes the main loop, it has an option to save parts of the main loop separately. You can use `Load` extension to continue the training or `continue_training` function.\n",
    "\n",
    "Note, that python doesn't know how to unpickle the objects from the global namespace if you are unpickling in a different script. One way to solve this problem is to define all your objects in some module. Other way is to run `continue_training` from the same script, in this case the objects from the global namespace defined in the same way when you run serialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blocks.extensions.saveload import Checkpoint\n",
    "# Reinitialize algorithm\n",
    "algorithm = GradientDescent(\n",
    "    cost=cost, parameters=cg.parameters,\n",
    "    step_rule=Scale(learning_rate=0.1))\n",
    "\n",
    "main_loop = MainLoop(\n",
    "        algorithm,\n",
    "        train_stream,\n",
    "        model=Model(cost),\n",
    "        extensions=[FinishAfter(after_n_batches=2), \n",
    "                    train_monitoring, test_monitoring,\n",
    "                    Checkpoint('mnist.pkl'), \n",
    "                    Printing(after_batch=True)])\n",
    "main_loop.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the log entry called 'saved_to'. You can now load the checkpoint using `load`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blocks.serialization import load\n",
    "loaded_main_loop = load('mnist.pkl')\n",
    "loaded_main_loop.model.parameters[0].get_value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file can be loaded with `numpy.load` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = numpy.load('mnist.pkl')\n",
    "params.keys()\n",
    "params['mlp-linear_0.W']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other extensions\n",
    "\n",
    "Blocks has a bunch of other useful extensions like\n",
    "- `Printing` -- prints whatever was added to the log\n",
    "- `Progressbar` -- outputs a progress bar of the training procedure\n",
    "- `TrackTheBest` -- checks if a quantity is the best so far and adds it to the log\n",
    "\n",
    "Blocks-extras contains more extensions, the most useful one is `Plot` which can create plots using Bokeh. The plots can be seen in your browser.\n",
    "\n",
    "###Predicates\n",
    "\n",
    "We have only one predicate now: `OnLogRecord` it triggers when a certain log record is found. Usually it is used in a couple with `TrackTheBest` extension. For example, you can save the model to a separate location when you get the best validation score.\n",
    "\n",
    "You can use `add_conditions` and `set_conditions` methods to modify conditions when to run the extension. Note, that `set_conditions` is going to overwrite the default ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blocks.extensions.predicates import OnLogRecord\n",
    "from blocks.extensions.training import TrackTheBest\n",
    "checkpoint = Checkpoint('mnist.pkl')\n",
    "extensions = [TrackTheBest('error_rate'), \n",
    "              checkpoint.add_condition(['after_epoch'],\n",
    "                  OnLogRecord('error_rate'),\n",
    "                  (\"mnist_best.pkl\",))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing your own extension\n",
    "Sometimes one needs to performs certain actions during the training process. You can implement your extension to do this job. You can inherit from `TrainingExtension` or from `SimpleExtension`.\n",
    "\n",
    "Using the first one you will need to implement one or several callbacks such as `before_batch`, `after_batch`, `every_n_epoch`, etc. To use the second one you need to implement `do` method and possibly add default callback names.\n",
    "\n",
    "You can access the main loop from the extension and therefore log, model, algorithm. If you need to access some other variable, just give a link to the constructor of the extension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercises\n",
    "\n",
    "Clone the blocks-examples [repository](https://github.com/mila-udem/blocks-examples) and open MNIST [example](https://github.com/mila-udem/blocks-examples/blob/master/mnist/__init__.py). \n",
    "\n",
    "### 1 Use other activation\n",
    "Change activation from tanh to ReLU in the MLP and add one more layer. You will need to find the ReLU brick and import it. Run the example for several iterations.\n",
    "\n",
    "### 2 Apply dropout\n",
    "Apply dropout with 0.1 drop probability to the input and with 0.5 to all other layers.\n",
    "\n",
    "### 3 Logging\n",
    "Save log separately (see documentation how to do it). Run a separate python notebook and unpickle the log. Install pandas and convert log to a pandas dataset (see log documentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
