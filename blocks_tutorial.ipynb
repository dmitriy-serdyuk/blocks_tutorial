{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Blocks tutorial\n",
    "For more information see [documentation](http://blocks.readthedocs.org/en/latest) and [examples](https://github.com/mila-udem/blocks-examples/).\n",
    "\n",
    "###Note\n",
    "Blocks is in early stage of development and it changes very fast. You can use a stable [version](https://github.com/mila-udem/blocks/releases/tag/v0.0.1) but the development version has much more functionality.\n",
    "\n",
    "We appreciate bug reports, pull requests, constructive criticism and answer questions. The mailing list can be found [here](https://groups.google.com/forum/#!forum/blocks-users).\n",
    "\n",
    "This tutorial mostly follows the MNIST [example](https://github.com/mila-udem/blocks-examples/blob/master/mnist/__init__.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Bricks\n",
    "\n",
    "###Introduction\n",
    "\n",
    "Blocks provides instruments to extend Theano. `Brick` is a parametrized Theano operation, bricks are used to construct Theano graphs.\n",
    "\n",
    "Bricks can be applied to Theano variables and output Theano variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import theano\n",
    "from theano import tensor\n",
    "from blocks.bricks import Linear\n",
    "x = tensor.matrix('features') # dim: (batch, features)\n",
    "linear = Linear(input_dim=784, output_dim=10)\n",
    "y_hat = linear.apply(x)\n",
    "y_hat = abs(2 * y_hat)\n",
    "isinstance(y_hat, theano.Variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compile a Theano function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from theano import function\n",
    "f = function([x], y_hat)\n",
    "f(numpy.zeros((10, 784)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function works like we expected except that the output is NaNs. The reason for this is that all the shared variables are initialized with NaN at the beginning. So, if your output is NaN, check that you didn't forget to initialize all the bricks properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blocks.initialization import Constant\n",
    "linear.weights_init = Constant(1.)\n",
    "linear.biases_init = Constant(0.)\n",
    "linear.initialize()\n",
    "f(numpy.zeros((10, 784)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every brick has a list of parameters and a list of its children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(linear.parameters)\n",
    "print(linear.children)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Linear` brick doesn't have any children. `MLP` is a sequence of linear transformations and activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blocks.bricks import MLP, Tanh, Softmax\n",
    "from blocks.initialization import IsotropicGaussian\n",
    "mlp = MLP([Tanh(), Softmax()], [784, 100, 10],\n",
    "          weights_init=IsotropicGaussian(0.01),\n",
    "          biases_init=Constant(0))\n",
    "mlp.initialize()\n",
    "probs = mlp.apply(tensor.flatten(x, outdim=2))\n",
    "mlp.children"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that activations and costs are also bricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blocks.bricks.cost import CategoricalCrossEntropy\n",
    "y = tensor.lmatrix('targets')\n",
    "cost = CategoricalCrossEntropy().apply(y.flatten(), probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Brick lifecycle\n",
    "The life-cycle of a brick is as follows:\n",
    "\n",
    "1. **Configuration:** set (part of) the *attributes* of the brick. Can take\n",
    "   place when the brick object is created, by setting the arguments of the\n",
    "   constructor, or later, by setting the attributes of the brick object. No\n",
    "   Theano variable is created in this phase.\n",
    "\n",
    "2. **Allocation:** (optional) allocate the Theano shared variables for the\n",
    "   *parameters* of the Brick. When `Brick.allocate` is called, the\n",
    "   required Theano variables are allocated and initialized by default to ``NaN``.\n",
    "\n",
    "3. **Application:** instantiate a part of the Theano computational graph,\n",
    "   linking the inputs and the outputs of the brick through its *parameters*\n",
    "   and according to the *attributes*. Cannot be performed (i.e., results in an\n",
    "   error) if the Brick object is not fully configured.\n",
    "\n",
    "4. **Initialization:** set the **numerical values** of the Theano variables\n",
    "   that store the *parameters* of the Brick. The user-provided value will\n",
    "   replace the default initialization value.\n",
    "\n",
    "####Note\n",
    "   If the Theano variables of the brick object have not been allocated when \n",
    "   `Application.apply` is called, Blocks will quietly call \n",
    "   `Brick.allocate`.\n",
    "\n",
    "For details see [this](http://blocks.readthedocs.org/en/latest/bricks_overview.html#bricks-life-cycle) tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Children bricks get the same initialization scheme as a parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp.children[0].weights_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change this behaviour by pushing the initialization configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp.push_initialization_config()\n",
    "mlp.linear_transformations[0].weights_init = Constant(1.)\n",
    "mlp.initialize()\n",
    "mlp.children[0].weights_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Graph filtering and modifications\n",
    "Using brick annotations one can easily extract variables from the computation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blocks.graph import ComputationGraph\n",
    "from blocks.filter import VariableFilter\n",
    "from blocks.roles import WEIGHT\n",
    "\n",
    "cg = ComputationGraph([cost])\n",
    "W1, W2 = VariableFilter(roles=[WEIGHT])(cg.variables)\n",
    "print(\"W1 brick:\", W1.tag.annotations[0].name)\n",
    "print(\"W2 brick:\", W2.tag.annotations[0].name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = cost + .00005 * (W1 ** 2).sum() + .00005 * (W2 ** 2).sum()\n",
    "cost.name = 'final_cost'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can filter by \n",
    "- roles of a variable,\n",
    "- bricks and applications which created a variable\n",
    "- variable names\n",
    "    \n",
    "Blocks assigns roles such as INPUT, OUTPUT, WEIGHT, BIAS, PARAMETER, AUXILIARY, etc.\n",
    "\n",
    "Variable filtering is a very powerfull tool. Usually it's used to extract variables for monitoring or apply different types of regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blocks.graph import apply_dropout\n",
    "cg_dropout = apply_dropout(cg, [W2], 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally `apply_dropout` uses `ComputationGraph.replace` which is a wrapper for Theano clone with replacement. It is smarter than Theano function and works with several replacements at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_cg = cg.replace({x: x + 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Main loop\n",
    "\n",
    "Main loop in Blocks manages the training process. All the additional functionality is added using extensions.\n",
    "\n",
    "Main loop fetches data from a datastream, feeds it to a training algorithm and runs extensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Algorithms\n",
    "\n",
    "Currently, there is only one algorithm: `GradientDescent`. It computes gradients and changes parameters according to a step rule which depends on the gradients.\n",
    "\n",
    "The step rule does most of the job: the simplies step rule is `Scale` which scales its input. The following code runs SGD with fixed learning rate `0.1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blocks.algorithms import GradientDescent, Scale\n",
    "algorithm = GradientDescent(\n",
    "    cost=cost, parameters=cg.parameters,\n",
    "    step_rule=Scale(learning_rate=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, step rule can be a sequence of several step rules. Each step rule in the sequence is applied to a preceding step rule output. Blocks has a set of predefined step rules:\n",
    "- `Scale` -- scales its input, if it is a single rule applied to gradients it is SGD\n",
    "- `CompositeRule` -- a rule used to compose several rules to a sequence\n",
    "- `Momentum` -- adds momentum (SGD-Momentum, but can be applied to any kind of rule)\n",
    "- `AdaDelta` -- adaptive learning rate AdaDelta algorithm\n",
    "- `RMSProp` -- another learning algorithm\n",
    "- `Adam` -- one more algorithm\n",
    "- `StepClipping` -- clips the step, can be used for gradient clipping if applied before or for step clipping if applied after other rules\n",
    "- and others, see [documentation](http://blocks.readthedocs.org/en/latest/api/algorithms.html)\n",
    "\n",
    "Here is an example of step composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from blocks.algorithms import CompositeRule, StepClipping\n",
    "gradient_clipping = CompositeRule([StepClipping(threshold=1.0), Scale(learning_rate=0.1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next example we'll need MNIST dataset from fuel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fuel.datasets.mnist import MNIST\n",
    "mnist_train = MNIST((\"train\",))\n",
    "mnist_test = MNIST((\"test\",))\n",
    "mnist_train.sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that we providently defined our input variables `x` and `y` with the same names as dataset sources. The main loop finds inputs by their names and feeds corresping dataset sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blocks.main_loop import MainLoop\n",
    "from blocks.model import Model\n",
    "from blocks.extensions import FinishAfter, Printing\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import SequentialScheme\n",
    "from fuel.transformers import Flatten\n",
    "\n",
    "train_stream = Flatten(\n",
    "    DataStream.default_stream(\n",
    "        mnist_train,\n",
    "        iteration_scheme=SequentialScheme(\n",
    "            mnist_train.num_examples, 50)),\n",
    "    which_sources=('features',))\n",
    "main_loop = MainLoop(\n",
    "        algorithm,\n",
    "        train_stream,\n",
    "        model=Model(cost),\n",
    "        extensions=[FinishAfter(after_n_batches=5), Printing()])\n",
    "\n",
    "main_loop.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple example uses Fuel to construct a data stream and runs training for `5` iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##Exercises\n",
    "\n",
    "Clone the blocks-examples [repository](https://github.com/mila-udem/blocks-examples) and open MNIST [example](https://github.com/mila-udem/blocks-examples/blob/master/mnist/__init__.py). \n",
    "\n",
    "###1 Use other activation\n",
    "Change activation from tanh to ReLU in the MLP and add one more layer. You will need to find the ReLU brick and import it. Run the example for several iterations.\n",
    "\n",
    "###2 Apply dropout\n",
    "Apply dropout with 0.8 drop probability to the input and with 0.5 to all other layers.\n",
    "\n",
    "###3 Logging\n",
    "Save log separately (see documentation how to do it). Run a separate python notebook and unpickle the log. Install pandas and convert log to a pandas dataset (see log documentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
