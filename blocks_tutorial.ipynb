{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Blocks tutorial\n",
    "For more information see [documentation](http://blocks.readthedocs.org/en/latest) and [examples](https://github.com/mila-udem/blocks-examples/).\n",
    "\n",
    "###Note\n",
    "Blocks is in early stage of development and it changes very fast. You can use a stable [version](https://github.com/mila-udem/blocks/releases/tag/v0.0.1) but the development version has much more functionality.\n",
    "\n",
    "We appreciate bug reports, pull requests, constructive criticism and answer questions. The mailing list can be found [here](https://groups.google.com/forum/#!forum/blocks-users).\n",
    "\n",
    "This tutorial mostly follows the MNIST [example](https://github.com/mila-udem/blocks-examples/blob/master/mnist/__init__.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Bricks\n",
    "\n",
    "###Introduction\n",
    "\n",
    "Bricks are \"parametrized ops\". They can build  Theano graphs given input variables.\n",
    "The process is called application of a brick. Let us create a simple ``Linear`` brick and apply it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import theano\n",
    "from theano import tensor\n",
    "from blocks.bricks import Linear\n",
    "x = tensor.matrix('features') # dim: (batch, features)\n",
    "linear = Linear(input_dim=784, output_dim=10)\n",
    "y_hat = linear.apply(x)\n",
    "y_hat = abs(2 * y_hat)\n",
    "isinstance(y_hat, theano.Variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compile a Theano function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from theano import function\n",
    "f = function([x], y_hat)\n",
    "f(numpy.zeros((10, 784)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function works like we expected except that the output is NaNs. The reason for this is that all the shared variables are initialized with NaN at the beginning. So, if your output is NaN, check that you didn't forget to initialize all the bricks properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blocks.initialization import Constant\n",
    "linear.weights_init = Constant(1.)\n",
    "linear.biases_init = Constant(0.)\n",
    "linear.initialize()\n",
    "f(numpy.zeros((10, 784)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every brick has a list of parameters and a list of its children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(linear.parameters)\n",
    "print(linear.children)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Linear` brick doesn't have any children. `MLP` is a sequence of linear transformations and activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blocks.bricks import MLP, Tanh, Softmax\n",
    "from blocks.initialization import IsotropicGaussian\n",
    "mlp = MLP([Tanh(), Softmax()], [784, 100, 10],\n",
    "          weights_init=IsotropicGaussian(0.01),\n",
    "          biases_init=Constant(0))\n",
    "mlp.initialize()\n",
    "probs = mlp.apply(tensor.flatten(x, outdim=2))\n",
    "mlp.children"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that activations and costs are also bricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blocks.bricks.cost import CategoricalCrossEntropy\n",
    "y = tensor.lmatrix('targets')\n",
    "cost = CategoricalCrossEntropy().apply(y.flatten(), probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Brick lifecycle\n",
    "The life-cycle of a brick is as follows:\n",
    "\n",
    "1. **Configuration:** set (part of) the *attributes* of the brick. Can take\n",
    "   place when the brick object is created, by setting the arguments of the\n",
    "   constructor, or later, by setting the attributes of the brick object. No\n",
    "   Theano variable is created in this phase.\n",
    "\n",
    "2. **Allocation:** (optional) allocate the Theano shared variables for the\n",
    "   *parameters* of the Brick. When `Brick.allocate` is called, the\n",
    "   required Theano variables are allocated and initialized by default to ``NaN``.\n",
    "\n",
    "3. **Application:** instantiate a part of the Theano computational graph,\n",
    "   linking the inputs and the outputs of the brick through its *parameters*\n",
    "   and according to the *attributes*. Cannot be performed (i.e., results in an\n",
    "   error) if the Brick object is not fully configured.\n",
    "\n",
    "4. **Initialization:** set the **numerical values** of the Theano variables\n",
    "   that store the *parameters* of the Brick. The user-provided value will\n",
    "   replace the default initialization value.\n",
    "\n",
    "####Note\n",
    "   If the Theano variables of the brick object have not been allocated when \n",
    "   `Application.apply` is called, Blocks will quietly call \n",
    "   `Brick.allocate`.\n",
    "\n",
    "For details see [this](http://blocks.readthedocs.org/en/latest/bricks_overview.html#bricks-life-cycle) tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Children bricks get the same initialization scheme as a parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp.children[0].weights_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change this behaviour by pushing the initialization configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp.push_initialization_config()\n",
    "mlp.linear_transformations[0].weights_init = Constant(1.)\n",
    "mlp.initialize()\n",
    "mlp.children[0].weights_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Graph filtering and modifications\n",
    "Using brick annotations one can easily extract variables from the computation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blocks.graph import ComputationGraph\n",
    "from blocks.filter import VariableFilter\n",
    "from blocks.roles import WEIGHT\n",
    "\n",
    "cg = ComputationGraph([cost])\n",
    "W1, W2 = VariableFilter(roles=[WEIGHT])(cg.variables)\n",
    "print(\"W1 brick:\", W1.tag.annotations[0].name)\n",
    "print(\"W2 brick:\", W2.tag.annotations[0].name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = cost + .00005 * (W1 ** 2).sum() + .00005 * (W2 ** 2).sum()\n",
    "cost.name = 'final_cost'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can filter by \n",
    "- roles of a variable,\n",
    "- bricks and applications which created a variable\n",
    "- variable names\n",
    "    \n",
    "Blocks assigns roles such as INPUT, OUTPUT, WEIGHT, BIAS, PARAMETER, AUXILIARY, etc.\n",
    "\n",
    "Variable filtering is a very powerfull tool. Usually it's used to extract variables for monitoring or apply different types of regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blocks.graph import apply_dropout\n",
    "cg_dropout = apply_dropout(cg, [W2], 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally `apply_dropout` uses `ComputationGraph.replace` which is a wrapper for Theano clone with replacement. It is smarter than Theano function and works with several replacements at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_cg = cg.replace({x: x + 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Main loop\n",
    "\n",
    "Main loop in Blocks manages the training process. All the additional functionality is added using extensions.\n",
    "\n",
    "Main loop fetches data from a datastream, feeds it to a training algorithm and runs extensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Algorithms\n",
    "\n",
    "Currently, there is only one algorithm: `GradientDescent`. It computes gradients and changes parameters according to a step rule which depends on the gradients.\n",
    "\n",
    "The step rule does most of the job: the simplies step rule is `Scale` which scales its input. The following code runs SGD with fixed learning rate `0.1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blocks.algorithms import GradientDescent, Scale\n",
    "algorithm = GradientDescent(\n",
    "    cost=cost, parameters=cg.parameters,\n",
    "    step_rule=Scale(learning_rate=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, step rule can be a sequence of several step rules. Each step rule in the sequence is applied to a preceding step rule output. Blocks has a set of predefined step rules:\n",
    "- `Scale` -- scales its input, if it is a single rule applied to gradients it is SGD\n",
    "- `CompositeRule` -- a rule used to compose several rules to a sequence\n",
    "- `Momentum` -- adds momentum (SGD-Momentum, but can be applied to any kind of rule)\n",
    "- `AdaDelta` -- adaptive learning rate AdaDelta algorithm\n",
    "- `RMSProp` -- another learning algorithm\n",
    "- `Adam` -- one more algorithm\n",
    "- `StepClipping` -- clips the step, can be used for gradient clipping if applied before or for step clipping if applied after other rules\n",
    "- and others, see [documentation](http://blocks.readthedocs.org/en/latest/api/algorithms.html)\n",
    "\n",
    "Here is an example of step composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from blocks.algorithms import CompositeRule, StepClipping\n",
    "gradient_clipping = CompositeRule([StepClipping(threshold=1.0), Scale(learning_rate=0.1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next example we'll need MNIST dataset from fuel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fuel.datasets.mnist import MNIST\n",
    "mnist_train = MNIST((\"train\",))\n",
    "mnist_test = MNIST((\"test\",))\n",
    "mnist_train.sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that we providently defined our input variables `x` and `y` with the same names as dataset sources. The main loop finds inputs by their names and feeds corresping dataset sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blocks.main_loop import MainLoop\n",
    "from blocks.model import Model\n",
    "from blocks.extensions import FinishAfter, Printing\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import SequentialScheme\n",
    "from fuel.transformers import Flatten\n",
    "\n",
    "train_stream = Flatten(\n",
    "    DataStream.default_stream(\n",
    "        mnist_train,\n",
    "        iteration_scheme=SequentialScheme(\n",
    "            mnist_train.num_examples, 50)),\n",
    "    which_sources=('features',))\n",
    "main_loop = MainLoop(\n",
    "        algorithm,\n",
    "        train_stream,\n",
    "        model=Model(cost),\n",
    "        extensions=[FinishAfter(after_n_batches=5), Printing()])\n",
    "\n",
    "main_loop.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple example uses Fuel to construct a data stream and runs training for `5` iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Logging\n",
    "In the example above the printing extension prints the values from the log. You can access the log like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "main_loop.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log is a dictionary which maps from iteration number to a dictionary of log records. Each record is a pair of record name and its value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Monitoring\n",
    "\n",
    "There are two types of monitoring in Blocks: training monitoring and data stream monitoring. The firt one computes the monitored values during the training and uses training batches. This type of monitoring almost doesn't take time. The data stream monitoring iterates over the dataset and computes the monitored quantities. It may take some time to iterate over a big dataset. \n",
    "\n",
    "The first type usually used for approximate train subset monitoring and the second one for validation/test subsets. However, you can use `DataStreamMonitoring` for the train subset to get not averaged values.\n",
    "\n",
    "The following example shows regular usage of monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from blocks.extensions.monitoring import DataStreamMonitoring, TrainingDataMonitoring\n",
    "from blocks.bricks.cost import MisclassificationRate\n",
    "\n",
    "error_rate = MisclassificationRate().apply(y.flatten(), probs).copy(name='error_rate')\n",
    "\n",
    "training_monitoring = TrainingDataMonitoring(\n",
    "    [cost, error_rate], prefix='train', after_batch=True)\n",
    "test_monitoring = DataStreamMonitoring(\n",
    "    [cost, error_rate], \n",
    "    Flatten(\n",
    "        DataStream.default_stream(\n",
    "            mnist_test,\n",
    "            iteration_scheme=SequentialScheme(\n",
    "                mnist_test.num_examples, 500)),\n",
    "        which_sources=('features',)), \n",
    "    prefix='test',\n",
    "    after_batch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to combine values you can use aggregations schemes. For example for the mean gradient norm you can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blocks.monitoring import aggregation\n",
    "aggregation.mean(algorithm.total_gradient_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally the values are aggregated using a shared theano variable. It means that the shape of the aggregated value should be constant. In a case if you would like to monitor all the activations, for example, make sure that your batch size is constant and the time series have the same length.\n",
    "\n",
    "Gradient descent and step rules have fields which contain variables which can be monitored: total gradient norm, total step norm, learning rate, momentum, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reinitialize algorithm\n",
    "algorithm = GradientDescent(\n",
    "    cost=cost, parameters=cg.parameters,\n",
    "    step_rule=Scale(learning_rate=0.1))\n",
    "\n",
    "main_loop = MainLoop(\n",
    "        algorithm,\n",
    "        train_stream,\n",
    "        model=Model(cost),\n",
    "        extensions=[FinishAfter(after_n_batches=5), \n",
    "                    training_monitoring, test_monitoring, \n",
    "                    Printing(after_batch=True)])\n",
    "main_loop.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Serialization\n",
    "We tried to make serialization to work with any kind of user code and to be easy enough deserialized on different types of hardware. We partly addressed these tasks using `cPickle` and saving Theano shared variables as numpy array with meta information (we use persistent ids internally). Serialization is being refactored now, so pay attention to the mailing list.\n",
    "\n",
    "`blocks.serialization` module provides funcitons `dump`, `sequre_dump`, `load`, and `continue_training`.\n",
    "\n",
    "`Checkpoint` is an extension which serializes the main loop, it has an option to save parts of the main loop separately. You can use `Load` extension to continue the training or `continue_training` function.\n",
    "\n",
    "Note, that python doesn't know how to unpickle the objects from the global namespace if you are unpickling in a different script. One way to solve this problem is to define all your objects in some module. Other way is to run `continue_training` from the same script, in this case the objects from the global namespace defined in the same way when you run serialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blocks.extensions.saveload import Checkpoint\n",
    "# Reinitialize algorithm\n",
    "algorithm = GradientDescent(\n",
    "    cost=cost, parameters=cg.parameters,\n",
    "    step_rule=Scale(learning_rate=0.1))\n",
    "\n",
    "main_loop = MainLoop(\n",
    "        algorithm,\n",
    "        train_stream,\n",
    "        model=Model(cost),\n",
    "        extensions=[FinishAfter(after_n_batches=5), \n",
    "                    training_monitoring, test_monitoring,\n",
    "                    Checkpoint('mnist.pkl'), \n",
    "                    Printing(after_batch=True)])\n",
    "main_loop.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Other extensions\n",
    "\n",
    "Blocks has a bunch of other usefull extensions like\n",
    "- `Printing` -- prints whatever was added to the log\n",
    "- `Progressbar` -- outputs a progress bar of the training procedure\n",
    "- `TrackTheBest` -- checks if a quantity is the best so far and adds it to the log\n",
    "Blocks-extras contains more extensions, the most usefull one is `Plot` which can create plots using Bokeh. The plots can be seen in your browser.\n",
    "\n",
    "###Predicates\n",
    "\n",
    "We have only one predicate now: `OnLogRecord` it triggers when a certain log record is found. Ususally it is used in a couple with `TrackTheBest` extension. For example, you can save the model to a separate location when you get the best validation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blocks.extensions.predicates import OnLogRecord\n",
    "from blocks.extensions.training import TrackTheBest\n",
    "checkpoint = Checkpoint('mnist.pkl')\n",
    "extensions = [TrackTheBest('error_rate'), \n",
    "              checkpoint.add_condition(['after_epoch'],\n",
    "                  OnLogRecord('error_rate'),\n",
    "                  (\"mnist_best.pkl\",))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Writing your own extension\n",
    "Sometimes one needs to performs certain actions during the training process. You can implement your extension to do this job. You can inherit from `TrainingExtension` or from `SimpleExtension`.\n",
    "\n",
    "Using the first one you will need to implement one or several callbacks such as `before_batch`, `after_batch`, `every_n_epoch`, etc. To use the second one you need to implement `do` method and possibly add default callback names.\n",
    "\n",
    "You can access the main loop from the extension and therefore log, model, algorithm. If you need to access some other variable, just give a link to the constructor of the extension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##Exercises\n",
    "\n",
    "Clone the blocks-examples [repository](https://github.com/mila-udem/blocks-examples) and open MNIST [example](https://github.com/mila-udem/blocks-examples/blob/master/mnist/__init__.py). \n",
    "\n",
    "###1 Use other activation\n",
    "Change activation from tanh to ReLU in the MLP and add one more layer. You will need to find the ReLU brick and import it. Run the example for several iterations.\n",
    "\n",
    "###2 Apply dropout\n",
    "Apply dropout with 0.8 drop probability to the input and with 0.5 to all other layers.\n",
    "\n",
    "###3 Logging\n",
    "Save log separately (see documentation how to do it). Run a separate python notebook and unpickle the log. Install pandas and convert log to a pandas dataset (see log documentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
